{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExKNGhnRWTIA"
   },
   "source": [
    "## Vanilla Policy Optimisation\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d5/vanilla_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Preliminary questions:\n",
    "- Run the script with the defaults parameters on the terminal\n",
    "- Explain from torch.distributions.categorical import Categorical\n",
    "- google gym python, why is it useful?\n",
    "- Policy gradient is model based or model free?\n",
    "- Is policy gradient on-policy or off-policy?\n",
    "\n",
    "Read all the code, then:\n",
    "- Complete the ... in the compute_loss function.\n",
    "- Use https://github.com/patrick-kidger/torchtyping to type the functions get_policy, get_action. You can draw inspiration from the compute_loss function.\n",
    "- Answer the questions\n",
    "\n",
    "\n",
    "Don't begin working on this algorithms if you don't understand the blog: https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\n",
    "\n",
    "This exercise is short, but you should aim to understand everything in this code. Simply completing the types is not sufficient. The important thing here is to have a good understanding of each line of code, as well as the policy gradient theorem that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w0LH_e-_WTIF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f28f3a79-6658-4b93-e6ee-8a96c79b5481",
    "ExecuteTime": {
     "end_time": "2023-08-28T10:39:25.821725961Z",
     "start_time": "2023-08-28T10:39:25.821194527Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install torchtyping\n",
    "#!pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-Ib-ZcpPWTIG",
    "ExecuteTime": {
     "end_time": "2023-08-28T10:45:35.827544926Z",
     "start_time": "2023-08-28T10:45:33.899063532Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gym # by openAI, standardized environments for RL\n",
    "# not maintained anymore, gymnasium is the newer one\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "# torchtyping\n",
    "from torchtyping import TensorType, patch_typeguard\n",
    "# checks at runtime for types\n",
    "from typeguard import typechecked\n",
    "\n",
    "patch_typeguard()  # use before @typechecked\n",
    "\n",
    "\n",
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    # Build a feedforward neural network. Basically having 4 as input layer for the state and 2 as output layer for the action\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "\n",
    "    # What does * mean here? Search for unpacking in python\n",
    "    # We want to unpack the list since sequential wants\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2,\n",
    "          epochs=50, batch_size=5000, render=False):\n",
    "\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    # create environment\n",
    "    env = gym.make(env_name)\n",
    "    # gym environments define observations we can get and also the actions\n",
    "    # we can take\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    # get observation space dimension\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    # get number of activation space\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # Core of policy network\n",
    "    # What should be the sizes of the layers of the policy network?\n",
    "    # obs dimension is 4, so the dimension of our states\n",
    "    # action dimension is 2, so two actions to take\n",
    "    # so we have one input layer where the state gets in and we get the\n",
    "    # action out\n",
    "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    # What is the shape of obs?\n",
    "    # We can have a batch in the beginning, but then just 4 since this is\n",
    "    # the dimension of our state space\n",
    "    @typechecked # To be typed\n",
    "    def get_policy(obs: TensorType[... , obs_dim]):\n",
    "        # Now we simply get a probability distribution on which action to\n",
    "        # take from our current state position. So logits is a scrore\n",
    "        # for which action to take, so our NN is our policy, the logits_net\n",
    "        # Warning: obs sometimes has a batch dimension, sometimes there is no such dimension\n",
    "        logits = logits_net(obs)\n",
    "        # Tip: Categorical is a convenient pytorch object which enable register logits (or a batch of logits)\n",
    "        # and then being able to sample from this pseudo-probability distribution with the \".sample()\" method.\n",
    "        # categrorial is useful to sample logits\n",
    "        # It is used to somehow change the output\n",
    "        # Create a probability distribution out of our outputs from the NN\n",
    "        # Like a softmax\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    # What is the shape of obs?\n",
    "    # We have just 4 since this is\n",
    "    # the dimension of our state space\n",
    "    @typechecked # To be typed\n",
    "    def get_action(obs: TensorType[obs_dim]):\n",
    "        # Here we take an action from our policy, and sample an action from it\n",
    "        # Sample to take one of the actions given on the probability\n",
    "        # distribusion that we earlier calculated out of the NN\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    # What is the shape of obs?\n",
    "    # Here we have a batch dimension, so 77 e.g. and then the dimension of\n",
    "    # our state space\n",
    "    # has the shape [77, 4]\n",
    "    @typechecked\n",
    "    def compute_loss(obs: TensorType[\"b\", obs_dim], acts: TensorType[\"b\"], rewards: TensorType[\"b\"]):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        # So we have our observation space tensor with states which are of 4 dimension\n",
    "        # We have our action vector, with batch as first part and the action value which was taken\n",
    "        # We have our reward tensor with the rewards per state, and the reward we got for that action and state\n",
    "\n",
    "        # rewards: a piecewise constant vector containing the total reward of each episode.\n",
    "\n",
    "        # Use the get_policy function to get the categorical object, then sample from it with the 'log_prob' method.â€¹\n",
    "        # So here we basically get the outcome of our policy for every\n",
    "        # state, so the outcome of our NN\n",
    "        # and since we also have the reward (which is kinda like our label)\n",
    "        # we can do a computation to calculate a loss\n",
    "\n",
    "        # So again\n",
    "        # we need whole batch of logits (so actions given states, so our\n",
    "        # probability), and of this we actually need the log\n",
    "        # pass actions to logprob such that it knows to which actions it\n",
    "        # refers, so the logits\n",
    "\n",
    "        # Then multiply to return\n",
    "\n",
    "        # And then take the mean since we Sum up and divide up\n",
    "        # Since we want to increase the reward, so we take minus so we\n",
    "        # actually to gradient ascent\n",
    "        logid = get_policy(obs).log_prob(acts)\n",
    "        return -(logid * rewards).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    # give it the parameter of the policy so the NN\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient.\n",
    "        # This is our reward, the weights\n",
    "        batch_rets = []         # for measuring episode returns # What is the return?\n",
    "        # returns the same thing as weights\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        # This is now for one trajectory and not for the whole batch\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            # so we pass a tensor to it of our obs\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            # here we do a step within our environment according to the\n",
    "            # action of our policy and thus the parameters of our NN,\n",
    "            # which was used to sample this action\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "            # as a return we get the new state, the reward\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "            # we do this a few times until episode is over\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                # Is the reward discounted?\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                # Why do we use a constant vector here?\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        # So we are done with one epoch now\n",
    "        # Now we want to calculate the gradient for our NN to update the policy\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # For this we want to calculate the loss in this specific way based\n",
    "        # on our state, action and rewards and then update our weights and\n",
    "        # thus our policy\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  acts=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  rewards=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        # After calculating the loss, we do the gradient step\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "              (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NxFYaAoPWTIK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01017ce4-8cf3-4331-a0f5-f47014fde5e4",
    "ExecuteTime": {
     "end_time": "2023-08-28T10:46:30.041261839Z",
     "start_time": "2023-08-28T10:46:09.752408163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 10.327 \t return: 14.500 \t ep_len: 14.500\n",
      "epoch:   1 \t loss: 10.420 \t return: 14.750 \t ep_len: 14.750\n",
      "epoch:   2 \t loss: 44.778 \t return: 63.000 \t ep_len: 63.000\n",
      "epoch:   3 \t loss: 23.943 \t return: 32.500 \t ep_len: 32.500\n",
      "epoch:   4 \t loss: 15.032 \t return: 19.667 \t ep_len: 19.667\n",
      "epoch:   5 \t loss: 11.608 \t return: 15.500 \t ep_len: 15.500\n",
      "epoch:   6 \t loss: 26.550 \t return: 29.500 \t ep_len: 29.500\n",
      "epoch:   7 \t loss: 13.175 \t return: 17.750 \t ep_len: 17.750\n",
      "epoch:   8 \t loss: 23.596 \t return: 28.000 \t ep_len: 28.000\n",
      "epoch:   9 \t loss: 25.860 \t return: 36.000 \t ep_len: 36.000\n",
      "epoch:  10 \t loss: 43.928 \t return: 64.000 \t ep_len: 64.000\n",
      "epoch:  11 \t loss: 15.997 \t return: 23.000 \t ep_len: 23.000\n",
      "epoch:  12 \t loss: 19.899 \t return: 25.000 \t ep_len: 25.000\n",
      "epoch:  13 \t loss: 21.465 \t return: 31.500 \t ep_len: 31.500\n",
      "epoch:  14 \t loss: 20.834 \t return: 30.500 \t ep_len: 30.500\n",
      "epoch:  15 \t loss: 20.667 \t return: 27.333 \t ep_len: 27.333\n",
      "epoch:  16 \t loss: 17.843 \t return: 25.000 \t ep_len: 25.000\n",
      "epoch:  17 \t loss: 20.383 \t return: 28.500 \t ep_len: 28.500\n",
      "epoch:  18 \t loss: 32.508 \t return: 45.000 \t ep_len: 45.000\n",
      "epoch:  19 \t loss: 21.744 \t return: 32.500 \t ep_len: 32.500\n",
      "epoch:  20 \t loss: 36.786 \t return: 55.000 \t ep_len: 55.000\n",
      "epoch:  21 \t loss: 23.169 \t return: 26.500 \t ep_len: 26.500\n",
      "epoch:  22 \t loss: 15.391 \t return: 21.333 \t ep_len: 21.333\n",
      "epoch:  23 \t loss: 26.434 \t return: 31.000 \t ep_len: 31.000\n",
      "epoch:  24 \t loss: 46.388 \t return: 71.000 \t ep_len: 71.000\n",
      "epoch:  25 \t loss: 18.498 \t return: 26.333 \t ep_len: 26.333\n",
      "epoch:  26 \t loss: 36.675 \t return: 56.000 \t ep_len: 56.000\n",
      "epoch:  27 \t loss: 30.174 \t return: 37.500 \t ep_len: 37.500\n",
      "epoch:  28 \t loss: 16.961 \t return: 26.000 \t ep_len: 26.000\n",
      "epoch:  29 \t loss: 57.979 \t return: 87.000 \t ep_len: 87.000\n",
      "epoch:  30 \t loss: 24.257 \t return: 33.000 \t ep_len: 33.000\n",
      "epoch:  31 \t loss: 19.938 \t return: 30.500 \t ep_len: 30.500\n",
      "epoch:  32 \t loss: 16.086 \t return: 21.667 \t ep_len: 21.667\n",
      "epoch:  33 \t loss: 29.108 \t return: 40.500 \t ep_len: 40.500\n",
      "epoch:  34 \t loss: 13.188 \t return: 20.000 \t ep_len: 20.000\n",
      "epoch:  35 \t loss: 51.797 \t return: 58.000 \t ep_len: 58.000\n",
      "epoch:  36 \t loss: 21.633 \t return: 35.500 \t ep_len: 35.500\n",
      "epoch:  37 \t loss: 21.814 \t return: 33.000 \t ep_len: 33.000\n",
      "epoch:  38 \t loss: 53.928 \t return: 83.000 \t ep_len: 83.000\n",
      "epoch:  39 \t loss: 37.878 \t return: 57.000 \t ep_len: 57.000\n",
      "epoch:  40 \t loss: 27.856 \t return: 42.500 \t ep_len: 42.500\n",
      "epoch:  41 \t loss: 15.142 \t return: 24.000 \t ep_len: 24.000\n",
      "epoch:  42 \t loss: 22.708 \t return: 35.500 \t ep_len: 35.500\n",
      "epoch:  43 \t loss: 29.882 \t return: 48.000 \t ep_len: 48.000\n",
      "epoch:  44 \t loss: 53.590 \t return: 86.000 \t ep_len: 86.000\n",
      "epoch:  45 \t loss: 51.647 \t return: 82.000 \t ep_len: 82.000\n",
      "epoch:  46 \t loss: 43.322 \t return: 71.000 \t ep_len: 71.000\n",
      "epoch:  47 \t loss: 51.653 \t return: 74.000 \t ep_len: 74.000\n",
      "epoch:  48 \t loss: 42.999 \t return: 43.333 \t ep_len: 43.333\n",
      "epoch:  49 \t loss: 51.391 \t return: 85.000 \t ep_len: 85.000\n"
     ]
    }
   ],
   "source": [
    "train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2,\n",
    "      epochs=50, batch_size=50, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-FOefgVWTIL"
   },
   "outputs": [],
   "source": [
    "# Original algo here: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/vpg/vpg.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML4G')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92168008c1991bbe6f37c0a293534e68a58b2d6f9d0a850eaf02432aa4f31239"
   }
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
